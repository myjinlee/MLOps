#!/usr/bin/env python
# coding: utf-8

# ## Libraries

# In[129]:


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random

# Sklearn
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, GridSearchCV
# Models (some libraries are used to model selection process)
"""
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBRegressor, XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
"""


# In[130]:


np.random.seed(42)
random.seed(42)


# ##Load dataset

# In[131]:


# read csv file
df = pd.read_csv(f"./survey_lung_cancer.csv")
print(df)


# In[132]:


df.head()


# ##missing value check

# In[133]:


# check the number of missing values for each column
missing_values = df.isnull().sum()

# ouput only columns with missing values
missing_values = missing_values[missing_values > 0]

print(missing_values)
# Check that there are no missing values â€‹â€‹in the print result


# In[134]:


df.info()


# ##Data distribution

# In[135]:


# numerical variable
numerical = ['AGE']

# categorical variable
categorical = [
    'GENDER', 'SMOKING', 'YELLOW_FINGERS', 'ANXIETY', 'PEER_PRESSURE',
    'CHRONIC DISEASE', 'FATIGUE ', 'ALLERGY ', 'WHEEZING',
    'ALCOHOL CONSUMING', 'COUGHING', 'SHORTNESS OF BREATH',
    'SWALLOWING DIFFICULTY', 'CHEST PAIN'
]
target = 'LUNG_CANCER'


# In[136]:


#look at numerical data distribution
for i in df[numerical].columns:
    plt.hist(df[numerical][i])
    plt.xticks()
    plt.xlabel(i)
    plt.ylabel('number of people')
    plt.show()


# In[137]:


#look at categorical data distribution except target
for idx, i in enumerate(df[categorical].columns):
    value_counts = df[categorical][i].value_counts()
    sns.barplot(x=value_counts.index, y=value_counts.values, palette=sns.color_palette("Set2", n_colors=len(value_counts)))
    plt.xlabel(i)
    plt.ylabel('number of people')
    plt.show()


# In[138]:


# target class distribution
value_counts_target = df[target].value_counts()
sns.barplot(x=value_counts_target.index, y=value_counts_target.values, palette=sns.color_palette("Set2", n_colors=len(value_counts_target)))
plt.xlabel(target)


# ## Transform the values

# - GENDER : M / F or M / W => 0 / 1
# - LUNG_CANCER :	YES / NO => 1 / 0
# - Other binary variables	YES / NO => 1 / 0

# In[139]:


# change the type of the valued ; string -> int not label encoding
#df.columns = [col.strip() for col in df.columns]  # remove empty space

# except fot AGE, GENDER, LUNG_CANCER columns, dealing with the rest columns
binary_cols = [col for col in df.columns if col not in ['AGE', 'GENDER', 'LUNG_CANCER']]

# transform the values ; 1(=NO), 2(=YES) -> 0(=NO), 1(=YES)
for col in binary_cols:
    df[col] = df[col].map({1: 0, 2: 1, '1': 0, '2': 1})  # also consider the case of containing string..

# GENDER: M â†’ 0, W â†’ 1
df['GENDER'] = df['GENDER'].astype(str).str.upper().map({'M': 0, 'F': 1})

# LUNG_CANCER: NO â†’ 0, YES â†’ 1
df['LUNG_CANCER'] = df['LUNG_CANCER'].astype(str).str.upper().map({'NO': 0, 'YES': 1})


# In[140]:


print(df)
df.info()


# ## Split the dataset for train and test

# In[141]:


# divide into properties(X) and target(y)
X = df.drop(columns=['LUNG_CANCER'])
y = df['LUNG_CANCER']

# Split Train/Test (85% train, 15% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)
# stratify=y ; Divide y by maintaining the class ratio on both train and test.


# In[142]:


# ê²°ê³¼ í™•ì¸
print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)
print("Train label distribution:\n", y_train.value_counts(normalize=True))
print("Test label distribution:\n", y_test.value_counts(normalize=True))


# In[143]:


print(X.columns)


# ## Training & evaluation
# - Before training and after feature selection, perform SMOTE
# - Optimization by tuning model parameters
# - Since it is a binary classification problem, the classifier model is considered.

# In[144]:


random_state = 42
kf = KFold(n_splits=10, shuffle=True, random_state=random_state)
scoring = "recall"

models = {}


# In[145]:


from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTENC
from supervised.automl import AutoML
from catboost import CatBoostClassifier
from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,
                             precision_score, recall_score,
                             precision_recall_curve, auc,
                             balanced_accuracy_score, matthews_corrcoef)
from collections import Counter

# categorical feature ì¸ë±ìŠ¤
categorical_indices = [i for i, col in enumerate(X_train.columns) if col!='AGE']
random_state = 42

# SMOTENC ì ìš© í›„ AutoML ì ìš©
smote = SMOTENC(categorical_features=categorical_indices, random_state=42, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)


# In[146]:


from supervised.automl import AutoML
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, average_precision_score
import pandas as pd

# ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ ëª©ë¡
metrics_explain = ["logloss", "f1", "auc", "accuracy", "average_precision"]

# í•™ìŠµëœ ëª¨ë¸, ì˜ˆì¸¡ ê²°ê³¼, ì„±ëŠ¥ ì €ì¥ìš© dict
automl_models_explain = {}
predictions_explain = {}
test_scores_explain = {}

for metric in metrics_explain:
    print(f"\nâ–¶ Training AutoML (Explain) with eval_metric = '{metric}'")

    # AutoML ì‹¤í–‰
    automl_explain = AutoML(
        mode="Explain", 
        total_time_limit=1000,
        results_path=f"AutoML_results_explain_{metric}",
        eval_metric=metric
    )
    automl_explain.fit(X_resampled, y_resampled)
    
    # ë¦¬í¬íŠ¸ ìƒì„± - README.html
    automl_explain.report()

    # ëª¨ë¸ ë° ì˜ˆì¸¡ ì €ì¥
    automl_models_explain[metric] = automl_explain
    preds = automl_explain.predict(X_test)
    predictions_explain[metric] = preds

    # ì‹¤ì œ test ì„±ëŠ¥ í‰ê°€
    try:
        auc_score = roc_auc_score(y_test, preds)
    except:
        auc_score = "N/A"

    try:
        ap_score = average_precision_score(y_test, preds)
    except:
        ap_score = "N/A"

    test_scores_explain[metric] = {
        "Accuracy": accuracy_score(y_test, preds),
        "F1 Score": f1_score(y_test, preds),
        "Precision": precision_score(y_test, preds),
        "Recall": recall_score(y_test, preds),
        "ROC AUC": auc_score,
        "Average Precision": ap_score
    }

# ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
df_explain_results = pd.DataFrame(test_scores_explain).T
print("\n Test Set Evaluation Results (Explain mode):")
display(df_explain_results)


# In[147]:


from supervised.automl import AutoML
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, average_precision_score
import pandas as pd

# ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ ëª©ë¡
metrics_compete = ["logloss", "f1", "auc", "accuracy", "average_precision"]

# í•™ìŠµëœ ëª¨ë¸ê³¼ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš© dict
automl_models_compete = {}
predictions_compete = {}
test_scores_compete = {}

for metric in metrics_compete:
    print(f"\nâ–¶ Training AutoML (Compete) with eval_metric = '{metric}'")
    
    # AutoML ì‹¤í–‰
    automl_compete = AutoML(
        mode="Compete",
        total_time_limit=2000,
        results_path=f"AutoML_results_compete_{metric}",
        eval_metric=metric
    )
    automl_compete.fit(X_resampled, y_resampled)
    
    # ë¦¬í¬íŠ¸ ìƒì„± - README.html
    automl_compete.report()

    # ëª¨ë¸ ë° ì˜ˆì¸¡ ì €ì¥
    automl_models_compete[metric] = automl_compete
    preds_compete = automl_compete.predict(X_test)
    predictions_compete[metric] = preds_compete

    # ì‹¤ì œ test ì„±ëŠ¥ í‰ê°€
    try:
        auc_score = roc_auc_score(y_test, preds_compete)
    except:
        auc_score = "N/A"

    try:
        ap_score = average_precision_score(y_test, preds_compete)
    except:
        ap_score = "N/A"

    test_scores_compete[metric] = {
        "Accuracy": accuracy_score(y_test, preds_compete),
        "F1 Score": f1_score(y_test, preds_compete),
        "Precision": precision_score(y_test, preds_compete),
        "Recall": recall_score(y_test, preds_compete),
        "ROC AUC": auc_score,
        "Average Precision": ap_score
    }

# ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
df_compete_results = pd.DataFrame(test_scores_compete).T
print("\n Test Set Evaluation Results (by metric):")
display(df_compete_results)


# ## Artifacts analysis

# In[148]:


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# ### Explain mode

# In[149]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import os
import glob

# ì—¬ëŸ¬ metric ì‹¤í—˜ ê²°ê³¼ê°€ ì €ì¥ëœ í´ë”ë“¤ ì§€ì • (ì˜ˆì‹œ)
results_folders = [
    "AutoML_results_explain_logloss",
    "AutoML_results_explain_f1",
    "AutoML_results_explain_auc",
    "AutoML_results_explain_accuracy",
    "AutoML_results_explain_precision",
    "AutoML_results_explain_recall"
]

for results_path in results_folders:
    if not os.path.exists(os.path.join(results_path, "leaderboard.csv")):
        print(f"[ X ] {results_path} â†’ leaderboard.csv ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    print(f"\n Analyzing results in: {results_path}\n")

    # 1. ë¦¬ë”ë³´ë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    leaderboard = pd.read_csv(os.path.join(results_path, "leaderboard.csv"))

    # 2. metric_type ì¢…ë¥˜ í™•ì¸ ë° í”¼ë²—
    metric_types = leaderboard["metric_type"].unique()
    pivot = leaderboard.pivot_table(index=["name", "model_type"], 
                                     columns="metric_type", 
                                     values="metric_value")
    print("Metric Table:")
    display(pivot)

    # 3. metric ë³„ ì„±ëŠ¥ ì‹œê°í™”
    for metric in metric_types:
        if metric in pivot.columns:
            pivot[metric].sort_values(ascending=False).plot(kind="bar", figsize=(10, 4), title=f"{results_path}: Model Comparison by {metric.upper()}")
            plt.ylabel(metric.upper())
            plt.tight_layout()
            plt.show()

    # 4. ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì‹œê°í™”
    importance_imgs = glob.glob(os.path.join(results_path, "*", "importance.png"))
    for img_path in importance_imgs:
        model_name = os.path.basename(os.path.dirname(img_path))
        img = Image.open(img_path)
        plt.figure()
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"{results_path} - {model_name} (Importance)")
        plt.show()

    # 5. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    corr_path = os.path.join(results_path, "correlation_matrix.csv")
    if os.path.exists(corr_path):
        corr_df = pd.read_csv(corr_path, index_col=0)
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_df, annot=True, cmap="coolwarm")
        plt.title(f"{results_path} - Feature Correlation")
        plt.tight_layout()
        plt.show()
    else:
        print(" ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì—†ìŒ.")


# ### Compete Mode

# In[150]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import os
import glob

# ğŸ“ Compete ëª¨ë“œì—ì„œ ì—¬ëŸ¬ metric ê¸°ì¤€ ì‹¤í—˜í•œ í´ë”ë“¤ ë‚˜ì—´
results_folders = [
    "AutoML_results_compete_logloss",
    "AutoML_results_compete_f1",
    "AutoML_results_compete_auc",
    "AutoML_results_compete_accuracy",
    "AutoML_results_compete_precision",
    "AutoML_results_compete_recall"
]

for results_path in results_folders:
    if not os.path.exists(os.path.join(results_path, "leaderboard.csv")):
        print(f"[ X ] {results_path} â†’ leaderboard.csv ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    print(f"\nCOMPETE MODE ë¶„ì„ ì¤‘: {results_path}\n")

    # 1. ë¦¬ë”ë³´ë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    leaderboard = pd.read_csv(os.path.join(results_path, "leaderboard.csv"))

    # 2. metric_type ì¢…ë¥˜ ë° í”¼ë²— í…Œì´ë¸” ìƒì„±
    metric_types = leaderboard["metric_type"].unique()
    pivot = leaderboard.pivot_table(index=["name", "model_type"], 
                                     columns="metric_type", 
                                     values="metric_value")
    print("Metric Table:")
    display(pivot)

    # 3. metricë³„ ì„±ëŠ¥ ì‹œê°í™”
    for metric in metric_types:
        if metric in pivot.columns:
            pivot[metric].sort_values(ascending=False).plot(
                kind="bar", figsize=(10, 4),
                title=f"{results_path} - Model Comparison by {metric.upper()}"
            )
            plt.ylabel(metric.upper())
            plt.tight_layout()
            plt.show()

    # 4. ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì‹œê°í™” (ìˆë‹¤ë©´)
    importance_imgs = glob.glob(os.path.join(results_path, "*", "importance.png"))
    for img_path in importance_imgs:
        model_name = os.path.basename(os.path.dirname(img_path))
        img = Image.open(img_path)
        plt.figure()
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"{results_path} - {model_name} (Importance)")
        plt.show()

    # 5. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì¶œë ¥ (ìˆë‹¤ë©´)
    corr_path = os.path.join(results_path, "correlation_matrix.csv")
    if os.path.exists(corr_path):
        corr_df = pd.read_csv(corr_path, index_col=0)
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_df, annot=True, cmap="coolwarm")
        plt.title(f"{results_path} - Feature Correlation")
        plt.tight_layout()
        plt.show()
    else:
        print("ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì—†ìŒ.")


# In[151]:


# metric_typeì— ì–´ë–¤ ì§€í‘œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
print("ì‚¬ìš©ëœ metric_typeë“¤:")
print(leaderboard['metric_type'].unique())


# In[152]:


print(leaderboard.columns)

